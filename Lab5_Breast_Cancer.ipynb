{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7fdeed7-9d78-4b77-a355-c6e1b72563c2",
      "metadata": {},
      "source": [
        "<center>\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode_vertical.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606070a7-cf73-406a-a8a5-e62ead84682c",
      "metadata": {},
      "source": [
        "# **Breast Cancer Investigation**\n",
        "\n",
        "# Lab 5. Model Evaluating and Refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ba79d8-5020-4e45-be66-2645625461a5",
      "metadata": {},
      "source": [
        "Estimated time needed: **45** minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e8fd81b-e382-445f-8e9d-a52ec9455842",
      "metadata": {},
      "source": [
        "## Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff4f48d-40b2-4e5d-9950-c1bcbb13a25a",
      "metadata": {},
      "source": [
        "In this lab, explore healthcare data analysis using Python and Pandas, with a focus on breast cancer. Import libraries, load the dataset, and dive into data pre-preparation. Learn about pipeline classification, Logistic Regression, cross-validation, accuracy assessment, addressing over-sampling issues, and ensemble techniques. Join this project to enhance your data analysis and machine learning skills while unlocking the potential of medical data models for improved breast cancer prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc33ecb-5cfa-4af7-8de9-fa8408c0490a",
      "metadata": {},
      "source": [
        "## Objectives\n",
        "\n",
        "After completing this lab you will be able to:\n",
        "\n",
        "* Download and prepare the dataset for analysis.\n",
        "* Conduct basic data analysis and exploratory data visualization.\n",
        "* Perform feature engineering and selection.\n",
        "* Build and evaluate machine learning classification models.\n",
        "* Create ensemble models by combining multiple classifiers.\n",
        "* Calculate accuracy and analyze errors of the models.\n",
        "* Implement a data analysis pipeline to streamline the process.\n",
        "* Demonstrate practical applications of classifiers and ensembles in a specific domain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b64de21d-cdcb-4c7f-a935-65f8589cb575",
      "metadata": {},
      "source": [
        "## Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a095ac3-0a92-4fcf-a3c0-3e7ed0154fb5",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "    <ol>\n",
        "        <li><a href=\"#materials_and_methods\">Materials and methods</a></li>\n",
        "        <li><a href=\"#import_libraries\">Import Libraries</a></li>\n",
        "        <li><a href=\"#load_the_dataset\">Load the DataSet</a></li>\n",
        "        <li><a href=\"#data_pre_preparation\">Data pre-preparation</a></li>\n",
        "        <li><a href=\"#pipeline_classification\">Pipeline Classification</a>\n",
        "             <ul>\n",
        "                <li><a href=\"#LogisticRegression\">LogisticRegression</a></li>\n",
        "                <li><a href=\"#cross_validation\">Cross-validation</a></li>\n",
        "                <li><a href=\"#accuracy\">Accuracy</a></li>\n",
        "            </ul>\n",
        "        </li>\n",
        "        <li><a href=\"#over_sampling_problem\">Over-sampling problem</a></li>\n",
        "        <li><a href=\"#ensemble_of_classifiers\">Ensemble of classifiers</a></li>\n",
        "        <li><a href=\"#conclusions\">Conclusions</a></li>\n",
        "    </ol>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed60c4e4-b2fe-4383-b7d8-20f284b44429",
      "metadata": {},
      "source": [
        "## 1. Materials and methods <p id=\"materials_and_methods\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6760aaa7-b923-4b16-94d3-15855cd65b7c",
      "metadata": {},
      "source": [
        "In this lab, we will learn how to download and pre-prepare data, classify and combine classifiers into an ensemble.\n",
        "This lab consists of the following steps:\n",
        "* Download data - download and display data from a file\n",
        "* Preliminary data preparation - preliminary analysis of data structure, change of data structure and tables\n",
        "* Pipeline classification - classification and analysis by grouping stages\n",
        "    * Logistic regression - classification and analysis of accuracy and errors using logistic regression\n",
        "    * Over-sampling problem - solve the problem of uneven distribution of data\n",
        "    * Ensemble of classifiers - study various classifiers and methods of combining them into an ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ac547f-cabe-4c40-b609-963270546a4b",
      "metadata": {},
      "source": [
        "The statistical data obtained from <a href=\"https://www.kaggle.com/datasets/gunesevitan/breast-cancer-metabric\">https://www.kaggle.com/datasets/gunesevitan/breast-cancer-metabric</a> under <a href=\"https://opendatacommons.org/licenses/odbl/1-0/\" target=\"_blank\">Database: Open Database, Contents: Â© Original Authors</a> license."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1803556-8b2f-42cb-90a0-fbaba1dea890",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "* [Python](https://www.python.org) - middle level\n",
        "* [Pandas](https://pandas.pydata.org) - middle level \n",
        "* [Matplotlib](https://matplotlib.org) - basic level\n",
        "* [SeaBorn](https://seaborn.pydata.org) - basic level\n",
        "* [Scikit-Learn](https://scikit-learn.org/stable/) - middle level "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40861399-1694-48fe-8705-f883784f1bd1",
      "metadata": {},
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c78c5c63-03c7-42f5-b66a-c45615109650",
      "metadata": {},
      "source": [
        "After completing this lab, you will be able to:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b68b2867-20ad-44da-9389-a0737476911d",
      "metadata": {},
      "source": [
        "* Download DataSet from * .csv files\n",
        "* Conduct basic data analysis\n",
        "* Calculate new and change column types\n",
        "* Divide the DataSet into training and test\n",
        "* Use different machine learning classification methods\n",
        "* Combine classifiers into ensemble\n",
        "* Calculate accuracy and analyze errors\n",
        "* Combine all stages of data analysis with Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b350bb2d-bf06-4a3d-8edc-d2352078bc7a",
      "metadata": {},
      "source": [
        "## 2. Import Libraries/Define Auxiliary Functions <p id=\"import_libraries\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e09f64f-67fe-443d-bae1-919c297e681c",
      "metadata": {},
      "source": [
        "Libraries such as Scikit-Learn, imbalanced-learn should be installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726a1a86-f240-4591-8e9e-4f1495799242",
      "metadata": {},
      "outputs": [],
      "source": [
        "conda install -c intel scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "529d6ae0-b36b-4b1a-a481-ccf631c519a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "conda install -c conda-forge imbalanced-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e9d961-e634-48b0-9332-68adf6eb17ed",
      "metadata": {},
      "source": [
        "Some libraries should be imported before you can begin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a639af52-ea86-4aa7-8f3b-9d52e1a37a08",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler                         \n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn import set_config\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "#Classifiers\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "024510f6-0b5c-44eb-87b2-b1f8a905a36f",
      "metadata": {},
      "source": [
        "Let's disable warnings by **[warnings.filterwarnings()](https://docs.python.org/3/library/warnings.html)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1b035b19-92bf-4428-8ba6-7b9e125cc8c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b147b114-4680-4cf6-816c-97dd3bad880a",
      "metadata": {},
      "source": [
        "Further specify the value of the precision parameter equal to 2 to display two decimal signs (instead of 6 as default) by and  **[pd.options.display](https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb5631a-a4f7-43e0-87f3-65bc7d5804b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e99825e7-abcd-4c06-8274-845e2367e4e7",
      "metadata": {},
      "source": [
        "## 3. Download data from a .csv file <p id=\"load_the_dataset\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "071c7809-b126-45f8-bcba-f72b1709baaa",
      "metadata": {},
      "source": [
        "The next step is to download the data file from the repository by **[read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)**.\n",
        "\n",
        "We will use the same DataSet like in previous lab. Therefore next some steps will be the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c804062e-4d2b-4f70-a388-8b68a0859cc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX036XEN/breast_cancer.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41562ac4-4248-4f9b-ae59-547a5a07774d",
      "metadata": {},
      "source": [
        "Now let's look at our DataSet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd25bc9d-85db-4988-b0db-79b7e3473571",
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8559dceb-1292-453e-ab86-6be5baf857d5",
      "metadata": {},
      "source": [
        "## 4. Data pre-preparation <p id=\"data_pre_preparation\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d982256-2529-44f0-9eb3-434c77f24881",
      "metadata": {},
      "source": [
        "Let's study DataSet. As you can see DataSet consist 2509 rows Ã 29 columns. As you can see DataSet consist information of different types. We should be sure that python recognized data types in the right way. To do this we shoul use **[pandas.info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html?highlight=info#pandas.DataFrame.info)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2062c027-cbdf-4086-add7-e3f2dbe2bc4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58dd5431-61db-4823-ac9f-c477b20df07e",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary><b>Click to see attribute information</b></summary>\n",
        "    \n",
        "Input features (column names):\n",
        "\n",
        "1. `Age at Diagnosis` - Age of the patient at diagnosis time (numeric)\n",
        "2. `Type of Breast Surgery` - Breast cancer surgery type (categorical: `Breast Conserving`, `Mastectomy`)\n",
        "3. `Cancer Type Detailed` - Detailed Breast cancer types (categorical: `Breast`, `Breast Angiosarcoma`, `Breast Invasive Ductal Carcinoma`, `Breast Invasive Lobular Carcinoma`, `Breast Invasive Mixed Mucinous Carcinoma`, `Breast Mixed Ductal and Lobular Carcinoma`, `Invasive Breast Carcinoma`, `Metaplastic Breast Cancer`)\n",
        "4. `Cellularity` - Cancer cellularity post chemotherapy, which refers to the amount of tumor cells in the specimen and their arrangement into clusters (categorical: `High`, `Low`, `Moderate`)\n",
        "5. `Chemotherapy` - Whether or not the patient had chemotherapy as a treatment (yes/no) (boolean)\n",
        "6. `Pam50 + Claudin-low subtype` - Pam 50: is a tumor profiling test that helps show whether some estrogen receptor-positive (ER-positive), HER2-negative breast cancers are likely to metastasize (when breast cancer spreads to other organs). (categorical: `Basal`, `Her2`, `LumA`, `LumB`, `NC`, `Normal`, `claudin-low`)\n",
        "7. `Cohort` - Cohort is a group of subjects who share a defining characteristic (numeric)\n",
        "8. `ER status measured by IHC` - To assess if estrogen receptors are expressed on cancer cells by using immune-histochemistry (a dye used in pathology that targets specific antigen, if it is there, it will give a color, it is not there, the tissue on the slide will be colored)(categorical: `Positve`, `Negative`)\n",
        "9. `ER Status` - Cancer cells are positive or negative for estrogen receptors (categorical: `Positve`, `Negative`)\n",
        "10. `Neoplasm Histologic Grade` - Determined by pathology by looking the nature of the cells, do they look aggressive or not (It takes a value from 1 to 3) (numeric).\n",
        "11. `HER2 status measured by SNP6` - To assess if the cancer positive for HER2 or not by using advance molecular techniques (Type of next generation sequencing) (categorical: `Gain`, `Loss`, `Neutral`, `Undef`)\n",
        "12. `Tumor Other Histologic Subtype` - Type of the cancer based on microscopic examination of the cancer tissue (categorical: `Ductal/NST`, `Lobular`, `Medullary`, `Metaplastic`, `Mixed`, `Mucinous`, `Other`, `Tubular/ cribriform`)\n",
        "13. `Hormone Therapy` - Whether or not the patient had hormonal as a treatment (yes/no) (boolean)\n",
        "14. `Integrative Cluster` - Molecular subtype of the cancer based on some gene expression (categorical: `1`, `2`, `3`, `4ER+`, `4ER-`, `5`, `6`, `7`,  `8`, `9`, `10`)\n",
        "15. `Primary Tumor Laterality` - Whether it is involving the right breast or the left breast (categorical: `Left`, `Right`)\n",
        "16. `Lymph nodes examined positive` - To take samples of the lymph node during the surgery and see if there were involved by the cancer (numeric)\n",
        "17. `Mutation Count` - Number of gene that has relevant mutations (numeric)\n",
        "18. `Nottingham prognostic index` - It is used to determine prognosis following surgery for breast cancer. Its value is calculated using three pathological criteria: the size of the tumour; the number of involved lymph nodes; and the grade of the tumour. (numeric)\n",
        "19. `Oncotree Code` - The OncoTree is an open-source ontology that was developed at Memorial Sloan Kettering Cancer Center (MSK) for standardizing cancer type diagnosis from a clinical perspective by assigning each diagnosis a unique OncoTree code (categorical: `BRCA`, `BREAST`, `IDC`, `ILC`, `IMMC`, `MBC`, `MDLC`, `PBS`)\n",
        "20. `PR Status` - Cancer cells are positive or negative for progesterone receptors (categorical: `Positve`, `Negative`)\n",
        "21. `Radio Therapy` - Whether or not the patient had radio as a treatment (yes/no) (boolean)\n",
        "22. `3-Gene classifier subtype` - Three Gene classifier subtype (categorical: `ER+/HER2- High Prolif`, `ER+/HER2- Low Prolif`, `ER-/HER2-`, `HER2+`)\n",
        "23. `Tumor Size` - Tumor size measured by imaging techniques (numeric)\n",
        "24. `Tumor Stage` - Stage of the cancer based on the involvement of surrounding structures, lymph nodes and distant spread (numeric)\n",
        "25. `Overall Survival (Years)` - Duration from the time of the intervention to death (numeric)\n",
        "26. `Relapse Free Status (Years)` - Absence of any signs or symptoms of cancer recurrence or metastasis after a patient has completed treatment for breast cancer. (numeric)\n",
        "27. `Nottingham prognostic index-binned` - (categorical)\n",
        "28. `Inferred Menopausal State-Post` - Whether the patient is post menopausal or not (numeric)\n",
        "29. `Relapse Free Status-Not Recurred` - Absence of any signs or symptoms of cancer recurrence or metastasis after a patient has completed treatment for breast cancer (numeric)\n",
        "\n",
        "\n",
        "Output feature (desired target):\n",
        "\n",
        "30. `Patient's Vital Status` - Patient's Vital Status (categorical: `Died of Disease`,`Died of Other Causes`, `Living`)\n",
        "    \n",
        "    </details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd391f12-3764-44dd-9273-6eb2786acb57",
      "metadata": {},
      "source": [
        "Let's study information of DataSet columns. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b029f120-cadc-4424-8133-9737c2002efb",
      "metadata": {},
      "source": [
        "## 5. Pipeline Classification <p id=\"pipeline_classification\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f0e60ca-1b00-458b-b048-d02f0e87cff5",
      "metadata": {},
      "source": [
        "### LogisticRegression <p id=\"LogisticRegression\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8c459a8-993f-48e6-bfb9-bdce49b0f6ea",
      "metadata": {},
      "source": [
        "Before classification, the dataset must be divided into input and target factors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd83a03-e24a-4fa5-a908-4839b766f1bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = df.drop(columns = [\"Patient's Vital Status\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc46958-dc8f-44d0-80cf-6c7c7721596e",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = df[\"Patient's Vital Status\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a521c3-a445-47ee-beb6-74b57ec274fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "x.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3856c08-456a-4d4d-b21c-61ebb86a0262",
      "metadata": {},
      "source": [
        "You can see the input data set consists from 28 columns.\n",
        "\n",
        "As you can see, 13 columns are categorical, and all other 15 - numerical. To make classification, all numerical fields must be normalized and categorical fields must be digitized. This can be automated using the **[sklearn.preprocessing.OrdinalEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)** and **[sklearn. preprocessing.StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)**.\n",
        "\n",
        "Since the machine learning process consists of several steps, each of which has the function `fit`,` predict` and etc, we can combine all these stages into one block using `Pipeline` (**[sklearn.pipeline.make_pipeline()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)**), **[sklearn.compose.make_column_transformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html)** and visualize it with: **[sklearn.set_config()](https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ceb4302-f520-456b-a778-927e3222c6e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_cat = list(x.select_dtypes(include=['object']).columns)\n",
        "col_num = list(x.select_dtypes(include=['float', 'int', 'bool']).columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424d0c9a-99d3-48c5-9b53-0e3e4d4c42b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "trans = make_column_transformer((OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),col_cat),\n",
        "                                (StandardScaler(),col_num),\n",
        "                                remainder = 'passthrough')\n",
        "set_config(display = 'diagram')\n",
        "trans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04cd2a3d-096b-4c29-b2f6-74303b957c6f",
      "metadata": {},
      "source": [
        "Next we must separate DataSets for train and test DataSets for calculate accuracy of models. To do this we can use **[sklearn.model_selection.train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)**. Let's separate DataSets in 0.3 proportion train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48286af6-9da8-4eb0-a9a1-8bcf63d6ae9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "826f6371-468e-419f-b8a9-83605911fe58",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4309ff-808f-4c3e-8252-9b73b7b3d8b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c91e8f5-a2e5-46b0-ad4b-cbea5ca63031",
      "metadata": {},
      "source": [
        "Nowe let's create a logistic regression model (**[sklearn.linear_model.LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)**) and add it to our `Pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d6b8b9f-3b6a-42a1-8e8a-74fceef1a670",
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = LogisticRegression()\n",
        "pipe_lr = make_pipeline(trans,lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb8eb0a-8d79-4353-b59b-bc44f35f468d",
      "metadata": {},
      "source": [
        "Let's fit our model and calculate its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f98abc9-bee4-4876-b356-c11c8bbcbf6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_lr.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e55bc186-7309-4d1d-a260-da3f0dcbce3c",
      "metadata": {},
      "source": [
        "### Cross-validation <p id=\"cross_validation\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c909000c-8775-4169-8d12-7afcebfc7a90",
      "metadata": {},
      "source": [
        "Cross-validation is a technique in machine learning where the available dataset is split into multiple subsets or folds, and the model is trained and tested on different subsets in a rotation. The primary purpose of cross-validation is to estimate how well the model is expected to perform when it is deployed to make predictions on new, unseen data.\n",
        "\n",
        "One common way to implement cross-validation is by using the cross_val_score helper function, which takes an estimator (the model to be trained and tested) and the dataset, and returns the scores from each fold. This allows for easy evaluation and comparison of different models based on their performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f08afa-2b97-49ed-99c3-b14fbf612599",
      "metadata": {},
      "outputs": [],
      "source": [
        "Rcross = cross_val_score(pipe_lr, x, y, cv=4)\n",
        "print([round(val, 2) for val in Rcross])\n",
        "print(\"The mean of the folds are\", round(Rcross.mean(), 2), \"and the standard deviation is\", round(Rcross.std(), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "676c751e-42bd-4766-baef-01ecb56e457b",
      "metadata": {},
      "source": [
        "Let's use `cross_val_predict` to generate cross-validated estimates for each input data point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09b08da-f03a-49de-a6e9-80c8050e1d6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = cross_val_predict(pipe_lr, x, y,cv=4)\n",
        "yhat[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a47161c-6f85-493b-a35b-87e39cd40c85",
      "metadata": {},
      "source": [
        "#### Accuracy <p id=\"accuracy\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0cbc11d-1033-4a25-acce-c1b3a045c158",
      "metadata": {},
      "source": [
        "Let's calculate accuracy of this pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b0bbd7-0dbf-46b8-9f4f-838a59e4cf06",
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_train = pipe_lr.score(x_train, y_train)\n",
        "scores_test = pipe_lr.score(x_test, y_test)\n",
        "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4e9de0-210f-464a-a4e7-5bd9a5233df2",
      "metadata": {},
      "source": [
        "Let's evaluate the correctness of the classification with: **[sklearn.metrics.plot_confusion_matrix()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)** and convince of these conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8942326f-5cb1-45cb-90d4-eb633c329cd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(pipe_lr, x_test, y_test)\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ae4f768-43e7-4444-aa08-f066d505a73b",
      "metadata": {},
      "source": [
        "As you can see from the table, our model predicts patient's vital status very well. At the same time, errors in the classification of patients that they will live are very big. The correct forecast is only 321 patients. In 184 cases when the patients actually will die of disease, the model shows that the patient will live. However, there are 0 cases where the model predicts that the patient will die of the disease, when in fact the patient will live.\n",
        "\n",
        "The `Recall` metric is used to assess the accuracy of only patients who will live: **[sklearn.metrics.recall_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec364ba-5906-4ed8-b844-527c598340d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_train = recall_score(y_train, pipe_lr.predict(x_train), average='macro')\n",
        "scores_test = recall_score(y_test, pipe_lr.predict(x_test), average='macro')\n",
        "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf7baf7-627d-4a5a-8ee9-436e59b773ce",
      "metadata": {},
      "source": [
        "As can be seen from this metric, the accuracy is very low. This means that in order to increase this metric of accuracy, it is necessary to increase the training sample. Let's analyze it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee47d455-9d98-4a5c-abdb-644f6c966083",
      "metadata": {},
      "source": [
        "### 6. Over-sampling problem <p id=\"over_sampling_problem\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37cfa031-5891-456d-a672-ed68324fe468",
      "metadata": {},
      "source": [
        "Let's analyze the Patient's Vital Status (**[seaborn.countplot()](https://seaborn.pydata.org/generated/seaborn.countplot.html)**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3f1b389-b4f0-4100-9086-e0976a3f56f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.countplot(x = y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6986b03-16c9-428c-b37b-c476b8fb7457",
      "metadata": {},
      "source": [
        "As you can see, the number of Living is much greater than the number of Died of Other Causes. To balance the data set, we can use a special function: **[imblearn.over_sampling.RandomOverSampler()](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html)**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b9a78a-f893-41f2-8888-a992a5079f8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "ROS = RandomOverSampler()\n",
        "o_x, o_y = ROS.fit_resample(x,y)\n",
        "sns.countplot(x = o_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7a8d5fa-f06d-4c04-b521-60049d526a52",
      "metadata": {},
      "source": [
        "Let's add this function to our `Pipeline`, fit the model and recalculate the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba260b6a-9d18-49e5-99da-84160a381e53",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_s_lr = make_pipeline(trans, ROS, lr)\n",
        "pipe_s_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a641b52e-eed5-484c-ac1d-a78a15404fce",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe_s_lr.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81969145-e235-455a-8ba7-7c7ad52d533e",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n",
        "<h1> Question  #1: </h1>\n",
        "\n",
        "<b>Calculate the precision for `pipe_s_lr` using the `Recall` metric.</b>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0b04369-f25a-47c4-a025-b7930cc9c673",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fa475b2-87ae-46fd-b203-5585431b1f07",
      "metadata": {},
      "source": [
        "<details><summary>Click here for the solution</summary>\n",
        "\n",
        "```python\n",
        "scores_train = recall_score(y_train, pipe_s_lr.predict(x_train), average='macro')\n",
        "scores_test = recall_score(y_test, pipe_s_lr.predict(x_test), average='macro')\n",
        "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33371e7b-e6e0-430a-9040-84ee1c37b3d8",
      "metadata": {},
      "source": [
        "As you can see, the balance for our dataset is barely changed.\n",
        "\n",
        "Let's analyze the model errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d97138c-e1bd-4232-9b22-f6da20b700bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_confusion_matrix(pipe_s_lr, x_test, y_test)  \n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cf9a4e-0f98-4314-9e3f-5f96a38a2f92",
      "metadata": {},
      "source": [
        "As we can see, the number of false predictions about a patient who will die has almost not changed. However, the error is high when the model predicts the patient's vital status. The `Precision` is used to assess this accuracy.\n",
        "\n",
        "To further increase the `Recall` metric, the model must be modified because the accuracy of logistic regression for unknown data is about the same as for known data. Therefore, it can no longer provide a better fit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ce5d975-010f-4c6e-948e-fa7b0bb589b2",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n",
        "<h1> Question  #2: </h1>\n",
        "\n",
        "<b>Ð¡alculate the cross-validation score for the new pipeline.</b>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ef1b42-0ed6-4ffb-b590-367ebd5899f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdf97d80-7fac-4384-8cd1-3deee33b6146",
      "metadata": {},
      "source": [
        "<details><summary>Click here for the solution</summary>\n",
        "\n",
        "```python\n",
        "Rcross = cross_val_score(pipe_s_lr, x, y, cv=4)\n",
        "print([round(val, 2) for val in Rcross])\n",
        "print(\"The mean of the folds are\", round(Rcross.mean(), 2), \"and the standard deviation is\", round(Rcross.std(), 2))\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45de9bd-edde-411d-896e-bbd8d77e4734",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n",
        "<h1> Question  #3: </h1>\n",
        "\n",
        "<b>Use `cross_val_predict` to generate cross-validated estimates for each input data point using the new pipeline.</b>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7bbaebd-96ac-4d2c-b443-85ed81c2c5ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e9048ef-f8f2-4165-a3dc-76942acdf62e",
      "metadata": {},
      "source": [
        "<details><summary>Click here for the solution</summary>\n",
        "\n",
        "```python\n",
        "yhat = cross_val_predict(pipe_s_lr, x, y,cv=4)\n",
        "yhat[0:5]\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca75df62-2ca6-41a1-a1ec-78bec04cbcd1",
      "metadata": {},
      "source": [
        "### 7. Ensemble of classifiers <p id=\"ensemble_of_classifiers\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c29871-f74b-4e25-a64d-665ad271d97c",
      "metadata": {},
      "source": [
        "Let's test other classifiers and compare the results.\n",
        "We will test:\n",
        "* [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression)\n",
        "* [Linear SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html?highlight=linear%20svm#sklearn.svm.LinearSVR)\n",
        "* [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier)\n",
        "* [Extra Tree](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
        "* [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)\n",
        "* [Multi-layer Perceptron classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html?highlight=mlpclassifier#sklearn.neural_network.MLPClassifier)\n",
        "* [Ada Boost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=adaboostclassifier#sklearn.ensemble.AdaBoostClassifier)\n",
        "* [Gradient Boosting for classification](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
        "* [Bagging classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f27b3b3-b742-408e-92d1-a92a15f8ee5c",
      "metadata": {},
      "source": [
        "Additionally, different classifiers may misclassify data in various circumstances. Therefore, model ensembles via Voting Classifier must be used in order to correct each other's errors.\n",
        "\n",
        "A **[Voting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)** is a machine learning model that gains experience by training on a collection of several models and forecasts an output (class) based on the class with the highest likelihood of being the output. To predict the output class based on the highest majority of votes, it merely averages the results of each classifier that was passed into the voting classifier. The concept is to build a single model that learns from these models and predicts output based on their aggregate majority of voting for each output class, rather than building separate dedicated models and determining the accuracy for each of them.\n",
        "\n",
        "Two different voting methods are supported by Voting Classifier.\n",
        "\n",
        "**Hard voting**: In hard voting, the projected output class is the one that had the greatest number of votes, i.e., the class that had the greatest likelihood of being predicted by each of the classifiers. In this case, the majority anticipated A as the output when three classifiers (A, A, and B) predicted the output class. Therefore, the final prediction will be A.\n",
        "\n",
        "**Soft Voting**: In a soft vote, the forecast for the output class is based on the likelihood assigned to that class on average. Assume that given some input, the prediction probabilities for classes A and B are (0.20, 0.32, 0.40) and (0.30, 0.47, 0.53), respectively. As a result, class A's average is 0.4333, while class B's average is 0.3067. As a result, class A is the winner because it had the highest probability as averaged by all classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727c2acd-fcda-433b-94ab-64fe44be23b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "names = [\"Logistic Regression\", \"Linear SVM\",\n",
        "         \"Decision Tree\", \"Extra Tree\", \"Random Forest\", \"Neural Net\", \n",
        "         \"AdaBoost\", \"GradientBoostingClassifier\", \"BaggingClassifier\", \"VotingClassifier\"]\n",
        "\n",
        "classifiers = [\n",
        "    LogisticRegression(),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    ExtraTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    AdaBoostClassifier(n_estimators=100, random_state=0),\n",
        "    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0),\n",
        "    BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0)]\n",
        "\n",
        "est = [(str(est), est) for est in classifiers]\n",
        "\n",
        "eclf = [VotingClassifier(\n",
        "     estimators=est,\n",
        "     voting='hard')]\n",
        "classifiers += eclf\n",
        "scores_train = []\n",
        "scores_test = []\n",
        "scores_train_s = []\n",
        "scores_test_s = []\n",
        "\n",
        "for name, classif in zip(names, classifiers):\n",
        "    print(name,'fitting.....')\n",
        "    clf = make_pipeline(trans, classif)\n",
        "    clf.fit(x_train,y_train)\n",
        "    score_train = recall_score(y_train, clf.predict(x_train), average='macro')\n",
        "    score_test = recall_score(y_test, clf.predict(x_test), average='macro')\n",
        "    scores_train.append(score_train)\n",
        "    scores_test.append(score_test)\n",
        "    \n",
        "    clf_s = make_pipeline(trans, ROS, classif)\n",
        "    clf_s.fit(x_train,y_train)\n",
        "    score_train_s = recall_score(y_train, clf_s.predict(x_train), average='macro')\n",
        "    score_test_s = recall_score(y_test, clf_s.predict(x_test), average='macro')\n",
        "    scores_train_s.append(score_train_s)\n",
        "    scores_test_s.append(score_test_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a725dece-575f-4fca-9f08-a2580874352b",
      "metadata": {},
      "source": [
        "Let's compare the accuracy of classifiers for balanced and unbalanced data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cef24c0e-2196-4aa0-86e2-d5e7a00bcb91",
      "metadata": {},
      "outputs": [],
      "source": [
        "res = pd.DataFrame(index = names)\n",
        "res['Train'] = np.array(scores_train)\n",
        "res['Test'] = np.array(scores_test)\n",
        "res['Train Over Sampler'] = np.array(scores_train_s)\n",
        "res['Test Over Sampler'] = np.array(scores_test_s)\n",
        "\n",
        "res.index.name = \"Classifier accuracy\"\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53a3f2e-f21b-4eb0-8075-91eec7df1c58",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(names, scores_test, label='Test')\n",
        "ax.bar(names, scores_test_s, label='Test Over Sampler')\n",
        "ax.legend(['Test', 'Test Over Sampler'])\n",
        "\n",
        "ax.set_title('Classifier Test Accuracies')\n",
        "ax.set_xlabel('Classifier')\n",
        "ax.set_ylabel('Accuracy')\n",
        "\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffcd5812-4ba7-4370-9bcd-5a1ce1d2de70",
      "metadata": {},
      "source": [
        "As you can see, the balanced data set leads to a sharp increase in accuracy in all classifiers. It can also be seen that the most accurate model was GradientBoostingClassifier. The ensemble of models showed better accuracy on the training data set and slightly worse on the test.\n",
        "\n",
        "Let's display the last classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d57893ee-0062-4122-a038-752e285d0fff",
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea675c2f-6d19-4f97-a453-855f19e10c0f",
      "metadata": {},
      "source": [
        "## 8. Conclusions <p id=\"conclusions\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c72c6f-b184-4c79-9af1-7df2b5be0454",
      "metadata": {},
      "source": [
        "In this lab we studied how to normalize numerical and categorical data. It was shown how to build training and test data sets. Shows how to fit different classifiers, evaluate their accuracy and analyze errors.\n",
        "We also studied how to join them together in an ensemble and create a model based on Pipeline.\n",
        "We compared the accuracy of different classifiers and their ensemble and showed how they can be used in medicine.\n",
        "\n",
        "The accuracy of the decision was about 70%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baf63afb-8572-4a2e-a152-bbdfafa41439",
      "metadata": {},
      "source": [
        "### Thank you for completing this lab!\n",
        "\n",
        "## Author\n",
        "\n",
        "<a href=\"https://author.skills.network/instructors/dmytro_shliakhovskyi\">Dmytro Shliakhovskyi</a>\n",
        "\n",
        "### Other Contributors\n",
        "\n",
        "<a href=\"https://author.skills.network/instructors/yaroslav_vyklyuk_2\">Prof. Yaroslav Vyklyuk, DrSc, PhD</a>\n",
        "\n",
        "<a href=\"https://author.skills.network/instructors/nataliya_boyko\">Ass. Prof. Nataliya Boyko, PhD</a>\n",
        "\n",
        "\n",
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                         |\n",
        "| ----------------- | ------- | ---------- | ---------------------------------------------------------- |\n",
        "|    2023-03-25     | 01 | Dmytro Shliakhovkyi | Lab created |\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
