{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "420bfc4f-a7a9-4e48-948c-167acdde6f33",
      "metadata": {},
      "outputs": [],
      "source": [
        "<center>\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode_vertical.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c451f700-f21e-44e3-9030-f9094da1ac6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# **Breast Cancer Investigation**\n\n# Lab 4. Classification of patients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e53debcf-3f1f-45b4-b59a-07db0f54a064",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Abstract\n\nImmerse yourself in healthcare data analysis with Python and Pandas in this lab focused on breast cancer. Learn to import libraries, prepare and transform data, select features using statistical techniques, and apply classification models like Decision Tree and Logistic Regression. Visualize the models. Enhance your data analysis and machine learning skills while contributing to breast cancer research."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ec5559-7986-42b0-a730-c3f948b8acd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb252b5-a24c-477a-b10c-0338788530fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ol>\n        <li><a href=\"#materials_and_methods\">Materials and Methods</a></li>\n        <li><a href=\"#import_libraries\">Import Libraries</a></li>\n        <li><a href=\"#load_the_dataset\">Load the Dataset</a></li>\n        <li><a href=\"#data_preparation\">Data preparation</a>\n            <ul>\n                <li><a href=\"#data_transformation\">Data transformation</a></li>\n                <li><a href=\"#encoding_and_normalization\">Encoding and Normalization</a></li>\n            </ul>\n        </li>\n        <li><a href=\"#features_selection\">Features selection</a>\n            <ul>\n                <li><a href=\"#chi_squared_statistic\">Chi-Squared Statistic</a></li>\n                <li><a href=\"#mutual_information_statistic\">Mutual Information Statistic</a></li>\n                <li><a href=\"#feature_importance\">Feature Importance</a></li>\n                <li><a href=\"#correlation_matrix_with_heatmap\">Correlation Matrix with Heatmap</a></li>\n            </ul>\n        </li>\n        <li><a href=\"#classification_models\">Classification Models</a></li>\n            <ul>\n                <li><a href=\"#decision_tree\">Decision Tree</a></li>\n                <li><a href=\"#extra_trees_classifier\">Extra Trees Classifier</a></li>\n                <li><a href=\"#logistic_regression\">Logistic Regression</a></li>\n                <li><a href=\"#visualization_of_decision_tree\">Visualization of Decision Tree</a></li>\n            </ul>\n        <li><a href=\"#conclusions\">Conclusions</a></li>\n    </ol>\n</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a8ccb96-231e-4e4f-8ec0-725057acacde",
      "metadata": {},
      "outputs": [],
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8623da3-c9cb-40b0-939f-250bc88a2520",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Materials and Methods <p id=\"materials_and_methods\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3619b269-6d84-4eda-9a73-733086bfc811",
      "metadata": {},
      "outputs": [],
      "source": [
        "Clinical and genomic data was downloaded from cBioPortal.\nhttps://www.kaggle.com/datasets/gunesevitan/breast-cancer-metabric\n\n> The Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) database is a Canada-UK Project which contains targeted sequencing data of 2,508 primary breast cancer samples.\n\nPredicting breast cancer is important because early detection can greatly increase the chances of successful treatment and recovery, ultimately improving the overall health and well-being of the patient.\n\nIn this lesson, we will try to give answers to a set of questions that may be relevant when breast cancer data:\n\n1. What are the most useful Python libraries for classification analysis?\n2. How to transform category data?\n3. How to create DataSet?\n4. How to do features selection?\n5. How to make, fit, and visualize the classification model?\n\nIn addition, we will make the conclusions from the obtained results of our classification analysis to predict the patient's vital status."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16c2bfa1-3f03-4996-9f9a-9123815e5fae",
      "metadata": {},
      "outputs": [],
      "source": [
        "[Scikit-learn](https://scikit-learn.org/stable/), often known as sklearn and formerly scikits.learn, is a free machine learning package for Python. Support vector machines, random forests, gradient boosting, k-means, and DBSCAN are just a few of the classification, regression, and clustering algorithms it offers. It is also built to work with the Python scientific and numerical libraries NumPy and SciPy."
      ]
    },
    {
      "cell_type": "code",
      "id": "e8b9cb2f-af59-4142-8451-b94867adad6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "!conda install --yes scikit-learn==0.24.2\n!conda install --yes python-graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d679aa7-7719-4ff1-90ed-df85622f816f",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Import Libraries <p id=\"import_libraries\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db261dde-65d8-4b8d-b374-d7ef71820458",
      "metadata": {},
      "outputs": [],
      "source": [
        "Import the libraries necessary to use in this lab. We can add some aliases to make the libraries easier to use in our code and set a default figure size for further plots. Ignore the warnings.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "b97af72c-3fae-4a46-9ada-34ba4735724c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (8, 6)\n# Data transformation\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n# Features Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, mutual_info_classif\n# Classificators\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn import tree\n# warnings deactivate\nimport warnings\nwarnings.filterwarnings('ignore')\nimport graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbf58b5-2ebe-4c88-add3-77e6a6966aed",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Load the Dataset <p id=\"load_the_dataset\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a89b931-4546-4298-ba41-11d66a27782b",
      "metadata": {},
      "outputs": [],
      "source": [
        "We will use the same DataSet as in previous labs. Therefore next some steps will be the same"
      ]
    },
    {
      "cell_type": "code",
      "id": "e48923e3-9064-4ad0-9a38-d04b50ea8cac",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0OKHEN/clean_df.csv', index_col=0)\ndf.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "id": "cff231f1-61ca-4098-952e-9a5132a669d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f533831-0533-4eb0-98ca-f210db7ba3ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see DataSet consists of 34 columns. The target column is: \"Patient's Vital Status\". Also, DataSet consists of 2509 rows. In previous labs, we investigated these columns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71fc5bdd-a809-418a-b826-d160da1f6404",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details>\n<summary><b>Click to see attribute information</b></summary>\nInput features (column names):\n\n1. `Patient ID` - Patient ID (object)\n2. `Age at Diagnosis` - Age of the patient at diagnosis time (numeric)\n3. `Type of Breast Surgery` - Breast cancer surgery type (categorical: `Breast Conserving`, `Mastectomy`)\n4. `Cancer Type Detailed` - Detailed Breast cancer types (categorical: `Breast`, `Breast Angiosarcoma`, `Breast Invasive Ductal Carcinoma`, `Breast Invasive Lobular Carcinoma`, `Breast Invasive Mixed Mucinous Carcinoma`, `Breast Mixed Ductal and Lobular Carcinoma`, `Invasive Breast Carcinoma`, `Metaplastic Breast Cancer`)\n5. `Cellularity` - Cancer cellularity post-chemotherapy, which refers to the number of tumor cells in the specimen and their arrangement into clusters (categorical: `High`, `Low`, `Moderate`)\n6. `Chemotherapy` - Whether or not the patient had chemotherapy as a treatment (yes/no) (boolean)\n7. `Pam50 + Claudin-low subtype` - Pam 50: is a tumor profiling test that helps show whether some estrogen receptor-positive (ER-positive) and HER2-negative breast cancers are likely to metastasize (when breast cancer spreads to other organs). (categorical: `Basal`, `Her2`, `LumA`, `LumB`, `NC`, `Normal`, `claudin-low`)\n8. `Cohort` - A cohort is a group of subjects who share a defining characteristic (numeric)\n9. `ER status measured by IHC` - To assess if estrogen receptors are expressed on cancer cells by using immune-histochemistry (a dye used in pathology that targets specific antigens, if it is there, it will give a color, it is not there, the tissue on the slide will be colored)(categorical: `Positive`, `Negative`)\n10. `ER Status` - Cancer cells are positive or negative for estrogen receptors (categorical: `Positive`, `Negative`)\n11. `Neoplasm Histologic Grade` - Determined by pathology by looking at the nature of the cells, do they look aggressive or not (It takes a value from 1 to 3) (numeric).\n12. `HER2 status measured by SNP6` - To assess if cancer is positive for HER2 or not by using advance molecular techniques (Type of next-generation sequencing) (categorical: `Gain`, `Loss`, `Neutral`, `Undef`)\n13. `HER2 Status` - Whether the cancer is positive or negative for HER2 (categorical: `Positive`, `Negative`)\n14. `Tumor Other Histologic Subtype` - Type of cancer-based on microscopic examination of the cancer tissue (categorical: `Ductal/NST`, `Lobular`, `Medullary`, `Metaplastic`, `Mixed`, `Mucinous`, `Other`, `Tubular/ cribriform`)\n15. `Hormone Therapy` - Whether or not the patient had hormonal as a treatment (yes/no) (boolean)\n16. `Integrative Cluster` - Molecular subtype of cancer-based on some gene expression (categorical: `1`, `2`, `3`, `4ER+`, `4ER-`, `5`, `6`, `7`,  `8`, `9`, `10`)\n17. `Primary Tumor Laterality` - Whether it is involving the right breast or the left breast (categorical: `Left`, `Right`)\n18. `Lymph nodes examined positive` - To take samples of the lymph node during the surgery and see if there were involved by cancer (numeric)\n19. `Mutation Count` - Number of a gene that has relevant mutations (numeric)\n20. `Nottingham prognostic index` - It is used to determine the prognosis following surgery for breast cancer. Its value is calculated using three pathological criteria: the size of the tumor; the number of involved lymph nodes; and the grade of the tumor. (numeric)\n21. `Oncotree Code` - The OncoTree is an open-source ontology that was developed at Memorial Sloan Kettering Cancer Center (MSK) for standardizing cancer type diagnosis from a clinical perspective by assigning each diagnosis a unique OncoTree code (categorical: `BRCA`, `BREAST`, `IDC`, `ILC`, `IMMC`, `MBC`, `MDLC`, `PBS`)\n22. `PR Status` - Cancer cells are positive or negative for progesterone receptors (categorical: `Positive`, `Negative`)\n23. `Radio Therapy` - Whether or not the patient had radio as a treatment (yes/no) (boolean)\n24. `3-Gene classifier subtype` - Three Gene classifier subtype (categorical: `ER+/HER2- High Prolif`, `ER+/HER2- Low Prolif`, `ER-/HER2-`, `HER2+`)\n25. `Tumor Size` - Tumor size measured by imaging techniques (numeric)\n26. `Tumor Stage` - Stage of cancer-based on the involvement of surrounding structures, lymph nodes, and distant spread (numeric)\n27. `Overall Survival (Years)` - Duration from the time of the intervention to death (numeric)\n28. `Relapse Free Status (Years)` - Absence of any signs or symptoms of cancer recurrence or metastasis after a patient has completed treatment for breast cancer. (numeric)\n29. `Nottingham prognostic index-binned` - (categorical)\n30. `Inferred Menopausal State-Post` - Whether the patient is post-menopausal or not (numeric)\n31. `Inferred Menopausal State-Pre` - Whether the patient is post-menopausal or not (numeric)\n32. `Relapse Free Status-Not Recurred` - Absence of any signs or symptoms of cancer recurrence or metastasis after a patient has completed treatment for breast cancer (numeric)\n33. `Relapse Free Status-Recurred` - Absence of any signs or symptoms of cancer recurrence or metastasis after a patient has completed treatment for breast cancer (numeric)\n\n\nOutput feature (desired target):\n\n34. `Patient's Vital Status` - Patient's Vital Status (categorical: `Died of Disease`,`Died of Other Causes`, `Living`)\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c1b3902-c680-4fc8-912a-61b4eaf4fa6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "Our goal is to create a classification model that can predict a Patient's Vital Status. To do this we must analyze and prepare data for such type of model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e6dd161-a87a-4be5-a279-e1e2bd6687e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Data preparation <p id=\"data_preparation\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f10c886-1a28-4402-a901-9511f065490c",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Data transformation <p id=\"data_transformation\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c02a2d8-1f18-4eca-bfaf-0931a6093d28",
      "metadata": {},
      "outputs": [],
      "source": [
        "First of all we should investigate how pandas recognized types of features"
      ]
    },
    {
      "cell_type": "code",
      "id": "d2592283-de79-4f86-b639-534c5a21b644",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de66808-0e93-4340-ae5d-10b94227ddc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see all categorical features were recognized like objects. We must change their type on \"categorical\". "
      ]
    },
    {
      "cell_type": "code",
      "id": "33b8202c-d75a-4a3a-be8a-0a125b011824",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_cat = list(df.select_dtypes(include=['object']).columns)\n# Remove the first column \"Patient ID\"\ncol_cat = col_cat[1:]\ncol_cat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c69c721-ca7a-4f19-a181-a991c1cb5b3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's look at the DataSet size."
      ]
    },
    {
      "cell_type": "code",
      "id": "1762cc13-fb62-43cb-8800-dc25d603458d",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.loc[:, col_cat] = df[col_cat].astype('category')\ndf.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f89c943c-6ff5-43b0-83ae-8b79bbe04d01",
      "metadata": {},
      "outputs": [],
      "source": [
        "To see the unique values of exact feature (column) we can use:"
      ]
    },
    {
      "cell_type": "code",
      "id": "e01d6b22-3545-45f8-a02c-62aa50d843e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Type of Breast Surgery'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f10fd1b-38e0-43e3-bf42-30354e40a722",
      "metadata": {},
      "outputs": [],
      "source": [
        "As was signed earlier the dataset contains 2509 objects (rows), for each of which 34 features are set (columns), including 1 target feature (Patient's Vital Status). 33 features, including the target, are categorical. These data types of values cannot use for classification. We must transform it to int or float.\nTo do this we can use **[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)** and **[OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)**. These functions can encode categorical features as an integer array.\n\nFirst of all we separate DataSet on input and output(target) DataSets"
      ]
    },
    {
      "cell_type": "code",
      "id": "39e0f930-194a-44c8-990c-7bd47292847d",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop([\"Patient's Vital Status\", \"Patient ID\"], axis=1) #input columns\ny = df[\"Patient's Vital Status\"] #target column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476041fa-f8cc-49e3-8018-27a5e00213db",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's create a list of boolean fields."
      ]
    },
    {
      "cell_type": "code",
      "id": "b56c91c9-9eff-4ce4-a760-76868cdce376",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_bool = list(df.select_dtypes(include=['bool']).columns)\ncol_bool"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "106efc41-82e5-41b7-8dcb-f82a6c1862fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Encoding and Normalization <p id=\"encoding_and_normalization\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2a4be7-c4da-4e2a-a91f-2a9bde57afa7",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's look at the value of the target column"
      ]
    },
    {
      "cell_type": "code",
      "id": "4b0d1a03-9be3-47f6-86ea-42a5681cf369",
      "metadata": {},
      "outputs": [],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7bc5a34-5d60-4678-9c6b-aa5e1c5573bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "Then create a list of categorical fields and transform their values into int arrays:"
      ]
    },
    {
      "cell_type": "code",
      "id": "b6cf1397-e2be-41d5-8cad-9621053851d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_cat = df.loc[:, ~df.columns.isin([\"Patient's Vital Status\", \"Patient ID\"])].select_dtypes(include=['category']).columns\noe = OrdinalEncoder()\noe.fit(X[col_cat])\nX_cat_enc = oe.transform(X[col_cat])"
      ]
    },
    {
      "cell_type": "code",
      "id": "84d53085-52e0-416b-bebf-bb00333c5687",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_cat_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4130d75-6d63-4f43-8119-1a1603736d1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Than we must transform arrays back into DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "id": "e013dc0c-4573-47c7-8b88-dc5202b04683",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_cat_enc = pd.DataFrame(X_cat_enc)\nX_cat_enc.columns = col_cat\nX_cat_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da561699-39d4-474f-b58e-87570425bd8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Numerical fields can have a different scale and can consist of negative values. These will lead to round mistakes and exceptions for some AI methods. To avoid it these features must be normalized.\n\nLet's create a list of numerical fields and normalize it using **[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)**"
      ]
    },
    {
      "cell_type": "code",
      "id": "6f5f5d13-905b-4e46-bd4b-737366a5a416",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_num = df.select_dtypes(include=['int64', 'float64']).columns\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_num_enc = scaler.fit_transform(X[col_num])"
      ]
    },
    {
      "cell_type": "code",
      "id": "3494333e-34fa-4c32-a376-479579345327",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_num_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520a3699-7407-4ac9-92b1-f947767c89bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "Like in the previous case transform back obtained arrays into DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "id": "9dc7dc98-240e-468b-bc5c-04451e99b64d",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_num_enc = pd.DataFrame(X_num_enc)\nX_num_enc.columns = col_num\nX_num_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a786909a-f6de-49ff-b550-999ea32494fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h3>Question  #1:</h3>\n\n<b>Transform the list of boolean and their values to int arrays: </b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "b93fc773-90cc-484b-b95b-70763f4beabc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n\noe = OrdinalEncoder()\noe.fit(X[col_bool])\nX_bool_enc = oe.transform(X[col_bool])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00197497-9659-4d6d-9a1a-ceec26c719d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\noe = OrdinalEncoder()\noe.fit(X[col_bool])\nX_bool_enc = oe.transform(X[col_bool])\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "code",
      "id": "d3eb9e53-b41f-4db4-8a0c-b09cf0e1ecca",
      "metadata": {},
      "outputs": [],
      "source": [
        "col_bool"
      ]
    },
    {
      "cell_type": "code",
      "id": "a50edba9-40aa-418d-ba6f-fb9bd16ac40e",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_bool_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012718ec-d7df-4011-a0a3-80a3de6bc223",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h3>Question  #2:</h3>\n\n<b>Transform arrays back into DataFrame: </b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "261c6101-886f-4de9-a4a8-25d51a633751",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n\nX_bool_enc = pd.DataFrame(X_bool_enc)\nX_bool_enc.columns = col_bool\nX_bool_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56450dc4-815b-4315-baa0-62bfb69934f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nX_bool_enc = pd.DataFrame(X_bool_enc)\nX_bool_enc.columns = col_bool\nX_bool_enc\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbde1d92-cec5-4cf5-bcc6-d54aa35ba44b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Then we should concatenate these DataFrames in one input DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "id": "ae055f5b-7840-4d61-bad4-798f5259f9a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_enc = pd.concat([X_cat_enc, X_num_enc, X_bool_enc], axis=1)\nx_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f34696-cdf0-4a42-90e1-8ab497523009",
      "metadata": {},
      "outputs": [],
      "source": [
        "The same transformation we must do for the target field"
      ]
    },
    {
      "cell_type": "code",
      "id": "77ada98a-f767-484d-a61f-15089f74d72a",
      "metadata": {},
      "outputs": [],
      "source": [
        "le = LabelEncoder()\nle.fit(y)\ny_enc = le.transform(y)\ny_enc = pd.Series(y_enc)\ny_enc.columns = y.name"
      ]
    },
    {
      "cell_type": "code",
      "id": "32ef8c29-8e08-4d9d-a3f6-25189be0c79c",
      "metadata": {},
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "id": "8cf943aa-b66d-42ab-9383-3f8e662723e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb974bf5-bd50-4576-9827-bb0666512ffd",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see values 'Died of Disease' was changed to 0, 'Died of Other Causes' to 1, and 'Living' to 2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af8c472-a8a7-46a1-91b7-9e48d3718ad9",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Features selection <p id=\"features_selection\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21eee31b-1852-443e-905e-1039bb56f204",
      "metadata": {},
      "outputs": [],
      "source": [
        "As was signed before input fields consist of 32 features. Of course, some of them are more significant for classification.\n\nTwo popular feature selection techniques can be used for categorical input data and a categorical (class) target variable.\n\nThey are:\n\n* Chi-Squared Statistic.\n* Mutual Information Statistic.\n\nLet’s take a closer look at each in turn.\n\nTo do this we can use **[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebfb39ff-e7bd-4f6c-b3de-e7bae5b70fd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Chi-Squared Statistic <p id=\"chi_squared_statistic\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be7f78ae-6aa4-4dc9-a6fb-f0ead624d923",
      "metadata": {},
      "outputs": [],
      "source": [
        "Pearson’s chi-squared statistical hypothesis test is an example of a test for independence between categorical variables.\n\nYou can learn more about this statistical test in the tutorial:\n\n[A Gentle Introduction to the Chi-Squared Test for Machine Learning](https://machinelearningmastery.com/chi-squared-test-for-machine-learning/)\nThe results of this test can be used for feature selection, where those features that are independent of the target variable can be removed from the dataset.\n\nThe scikit-learn machine library provides an implementation of the chi-squared test in the **[chi2()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2)** function. This function can be used in a feature selection strategy, such as selecting the top k most relevant features (largest values) via the SelectKBest class.\n\nFor example, we can define the SelectKBest class to use the chi2() function and select all (or most significant) features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7ab059e-f9c3-4d58-b7dc-428d642aad00",
      "metadata": {},
      "outputs": [],
      "source": [
        "Apply SelectKBest class to extract top 10 best features"
      ]
    },
    {
      "cell_type": "code",
      "id": "bace78d7-c21c-4b3a-afb4-3da06836a4af",
      "metadata": {},
      "outputs": [],
      "source": [
        "bestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(x_enc, y_enc)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ff30060-9eed-489f-a780-aaada1130845",
      "metadata": {},
      "outputs": [],
      "source": [
        "concat two DataFrames for better visualization "
      ]
    },
    {
      "cell_type": "code",
      "id": "fdfde5ce-690c-4dfe-84a0-107328496d4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "featureScores = pd.concat([dfcolumns, dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(round(featureScores.nlargest(10,'Score'),2))  #print 10 best features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ffe4ea2-ead6-4905-be69-66d36f569c89",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Mutual Information Statistic <p id=\"mutual_information_statistic\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da450d0-684c-412e-986e-90aed988510b",
      "metadata": {},
      "outputs": [],
      "source": [
        "Mutual information from the field of information theory is the application of information gain (typically used in the construction of decision trees) to feature selection.\n\nMutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.\n\n[You can learn more about mutual information in the following tutorial.](https://machinelearningmastery.com/information-gain-and-mutual-information)\n\nThe scikit-learn machine learning library provides an implementation of mutual information for feature selection via the **[mutual_info_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)** function.\n\nLike chi2(), it can be used in the SelectKBest feature selection strategy (and other strategies)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58eb52f9-923c-47d3-829b-95868d879deb",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h3>Question  #3:</h3>\n\n<b>Define the SelectKBest class to use the mutual_info_classif() function and select all (or most significant) features. </b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "d3b78940-48a5-4c1e-a4b5-6282b96ab676",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n\nbestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)\nfit = bestfeatures.fit(x_enc, y_enc)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns, dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(round(featureScores.nlargest(10,'Score'),2))  #print 10 best features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97df159e-247c-4a02-a8c5-7e23677193e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\nbestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)\nfit = bestfeatures.fit(x_enc, y_enc)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns, dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(round(featureScores.nlargest(10,'Score'),2))  #print 10 best features\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6a050e-8a4a-42b8-9cc1-1ef1523b0b41",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see these 2 functions select different significant features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b73dfbc6-ea1f-4f05-a55c-3241b206539b",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Feature Importance <p id=\"feature_importance\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2f52d0e-e1a5-4f67-8231-2ad6b6be7fb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can get the feature importance of each feature of your DataFrame by using the feature importance property of the exact classification model.\nFeature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\nFor example:\nFeature importance is an inbuilt class that comes with **[Tree Based Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)**, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c75c3982-fa81-4073-9478-beb1a39acecf",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's create and fit the model:"
      ]
    },
    {
      "cell_type": "code",
      "id": "89f69e9e-865a-486a-9400-9727dcbd5fd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ExtraTreesClassifier()\nmodel.fit(x_enc, y_enc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "592f3720-d327-4268-a019-d255bc11d926",
      "metadata": {},
      "outputs": [],
      "source": [
        "use inbuilt class feature_importances of tree-based classifiers"
      ]
    },
    {
      "cell_type": "code",
      "id": "e2c146a6-484e-4abf-ae12-1720c9e12ecf",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60b093f-637c-4bd5-a1d5-1fa5f922a602",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's transform it into a Series and plot a graph of feature importances for better visualization"
      ]
    },
    {
      "cell_type": "code",
      "id": "e9f6a3c2-e2b7-4f9d-a373-2a67e07bc4bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "feat_importances = pd.Series(model.feature_importances_, index=x_enc.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a69dba5-b0af-4393-a090-9f43aec5dc84",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can see that for the Extra Tree Classifier importance of features are different than in previous cases. It means that there are no exact rules for features selection. And their importance strictly dependence on model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3186f34-f76d-4893-be17-d4b2c649e19f",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Correlation Matrix with Heatmap <p id=\"correlation_matrix_with_heatmap\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f635466e-9297-40da-86e8-a5a8bf09d223",
      "metadata": {},
      "outputs": [],
      "source": [
        "Correlation states how the features are related to each other.\nCorrelation can be positive (an increase in one value of a feature increases the value of the other variable) or negative (an increase in one value of a feature decreases the value of the other variable)\nHeatmap makes it easy to identify which features are most related to the other variable, we will plot heatmap of correlated features using the Seaborn library."
      ]
    },
    {
      "cell_type": "code",
      "id": "5fbea048-177a-4596-9a50-590f3dde02f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "corrmat = x_enc.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\ng=sns.heatmap(x_enc[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac61c19d-39d1-4dc5-9e50-68980dc6b811",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see fields 'Inferred Menopausal State-Post', 'Inferred Menopausal State-Pre', 'Relapse Free Status-Not Recurred', 'Relapse Free Status-Recurred' and 'HER2 status measured by SNP6', 'HER2 Status' strictly correlate each other. It means that three of them must be removed from the calculation because there are linear dependencies between them. If we know one of them we can easily calculate another three. Let's remove 'Inferred Menopausal State-Pre', 'Relapse Free Status-Recurred', and 'HER2 Status'."
      ]
    },
    {
      "cell_type": "code",
      "id": "fa7514a4-e7a6-4e9a-a3d2-327cd03e2ebc",
      "metadata": {},
      "outputs": [],
      "source": [
        "col = x_enc.columns\ncol"
      ]
    },
    {
      "cell_type": "code",
      "id": "62560e92-57f1-44fc-91cc-453fc6859b62",
      "metadata": {},
      "outputs": [],
      "source": [
        "col = ['Type of Breast Surgery', 'Cancer Type Detailed', 'Cellularity',\n       'Pam50 + Claudin-low subtype', 'ER status measured by IHC',\n       'HER2 status measured by SNP6', 'Tumor Other Histologic Subtype',\n       'Integrative Cluster', 'Primary Tumor Laterality', 'Oncotree Code',\n       'PR Status', '3-Gene classifier subtype',\n       'Nottingham prognostic index-binned', 'Age at Diagnosis',\n       'Cohort', 'Neoplasm Histologic Grade',\n       'Lymph nodes examined positive', 'Mutation Count',\n       'Nottingham prognostic index', 'Tumor Size', 'Tumor Stage',\n       'Overall Survival (Years)', 'Relapse Free Status (Years)',\n       'Inferred Menopausal State-Post', 'Relapse Free Status-Recurred',\n       'Chemotherapy', 'Hormone Therapy', 'Radio Therapy']"
      ]
    },
    {
      "cell_type": "code",
      "id": "502fca26-5ff8-4918-b9ce-ad8116102949",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_enc = x_enc[col]"
      ]
    },
    {
      "cell_type": "code",
      "id": "9031b1e0-bfab-4ce4-8501-01a0f0a2b17d",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_enc"
      ]
    },
    {
      "cell_type": "code",
      "id": "0e7af2f8-2253-497c-bf30-da29d0c21245",
      "metadata": {},
      "outputs": [],
      "source": [
        "col"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85e5bb4d-dc0a-4aba-bccc-f12029b119cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Classification models <p id=\"classification_models\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae2246d4-ce1d-4bfa-8d75-4318cdc8d3e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Decision tree <p id=\"decision_tree\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838cf353-320c-48ee-a7df-97668ac0a00f",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e657c48-9594-43b7-b239-b7a04b9d3070",
      "metadata": {},
      "outputs": [],
      "source": [
        "The biggest drawback is the inability to visualize or justify the decision."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77776896-2789-419d-adf5-f9d170ffad0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "Decision trees are a popular supervised learning method for a variety of reasons. The benefits of decision trees include that they can be used for both regression and classification, they don’t require feature scaling, and they are relatively easy to interpret as you can visualize decision trees. This is not only a powerful way to understand your model, but also to communicate how your model works. Consequently, it would help to know how to make a visualization based on your model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50222845-1431-4226-a7e8-8ce2a0c53a59",
      "metadata": {},
      "outputs": [],
      "source": [
        "A **[Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)** is a supervised algorithm used in machine learning. It is using a binary tree graph (each node has two children) to assign for each data sample a target value. The target values are presented in the tree leaves. To reach the leaf, the sample is propagated through nodes, starting at the root node. In each node, a decision is made, as to which descendant node it should go. A decision is made based on the selected sample’s features. Decision Tree learning is a process of finding the optimal rules in each internal tree node according to the selected metric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671e6d2f-97d1-42c4-ad64-4ae69031c556",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's calculate the feature importance, choose the top 10 features, refit the model, and visualize the decision tree continuously."
      ]
    },
    {
      "cell_type": "code",
      "id": "22ed868f-545d-4b9b-a27b-84bab5524277",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dec = DecisionTreeClassifier()\nmodel_dec.fit(x_enc, y_enc)\nyhat = model_dec.predict(x_enc)\naccuracy = accuracy_score(y_enc, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1090741-2e4b-44d8-ae04-e471ea1c21de",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's create a user function that calculates the accuracy of a defined classifier model"
      ]
    },
    {
      "cell_type": "code",
      "id": "c4545614-a5ff-4766-89ad-e4f76ede3435",
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_ac(x, y, clf):\n    model_dec.fit(x, y)\n    yhat = model_dec.predict(x)\n    accuracy = accuracy_score(y, yhat)\n    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8ecfb0-92b2-4657-9fe0-48a1891535a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now, we need to create a user function that will calculate feature importance of a defined classifier model"
      ]
    },
    {
      "cell_type": "code",
      "id": "96d15356-d534-4ccb-a9dd-74a78eb3bcd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_imp(x, y, model_dec):\n    feat_importances = pd.Series(model_dec.feature_importances_, index=x.columns)\n    return feat_importances.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7daa01f-11cd-4835-97af-3e40cd32f098",
      "metadata": {},
      "outputs": [],
      "source": [
        "We can see features sorted by importance in descending order."
      ]
    },
    {
      "cell_type": "code",
      "id": "89603f81-f725-48e3-be6a-fead37acde1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "imp = model_imp(x_enc, y_enc, model_dec)\nprint(imp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd1f72d0-6a92-496c-8c6f-2f82487ce2d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "Plot graph of feature importances for better visualization"
      ]
    },
    {
      "cell_type": "code",
      "id": "d531e722-c0f7-4f4a-aea4-386eb452fb63",
      "metadata": {},
      "outputs": [],
      "source": [
        "imp.nlargest(10).plot(kind='barh')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072e50e9-f938-4d13-9fc6-25417e931797",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's build a plot that shows the accuracy of a defined model as a function of the number of input features"
      ]
    },
    {
      "cell_type": "code",
      "id": "d6c24a58-dcf0-45fc-a003-4c1224f7b942",
      "metadata": {},
      "outputs": [],
      "source": [
        "col = []\nac = []\nfor c in imp.index:\n    col.append(c)\n    ac.append(model_ac(x_enc[col], y_enc, model))\n    print('Input fields: ', len(col), 'Accuracy: %.2f' % (ac[-1]*100))\nac = pd.DataFrame(ac)\nac.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11047861-7d0f-4873-b1f9-546c2346d4a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "We can see that 3 features is enough to make 100% accuracy. So let's create list of this 3 features in order to use them for our next classification models."
      ]
    },
    {
      "cell_type": "code",
      "id": "58556a1b-56e4-4348-b14a-62f9990d33d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "col = imp.nlargest(3).index\ncol"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "203ec4e2-1ed7-4c12-96ad-5e7ef584d94a",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's refit the model on most important features"
      ]
    },
    {
      "cell_type": "code",
      "id": "599c8275-bb27-4b4f-b8ba-91afd11d894a",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_most_imp = x_enc[col]\nmodel_dec.fit(X_most_imp, y)\nyhat = model_dec.predict(X_most_imp)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b7f16f0-7572-4547-8d24-a5160298b7f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Extra Trees Classifier <p id=\"extra_trees_classifier\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "970a1cb5-b9f0-4500-87b0-d72edd315545",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's create and fit ExtraTreesClassifier on train DataSet and calculate the accuracy of classification:"
      ]
    },
    {
      "cell_type": "code",
      "id": "f7102012-538c-4d65-8d80-810ec0aaa872",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ExtraTreesClassifier()\nmodel.fit(X_most_imp, y_enc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee37c88-cb35-4347-a2ef-0d635e696a02",
      "metadata": {},
      "outputs": [],
      "source": [
        "Evaluate the model on test data to obtain predictions"
      ]
    },
    {
      "cell_type": "code",
      "id": "d4b31ef3-0f97-4a66-98e2-9745d8693fd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = model.predict(X_most_imp)\nprint(yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "584d364f-e7b8-48ca-a673-8f5f1350d1ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "Evaluate accuracy: "
      ]
    },
    {
      "cell_type": "code",
      "id": "b87de357-8191-4a6c-a075-e173cf0afa3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_enc, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "id": "14a4d355-e2a9-493c-a4bd-86270a8423df",
      "metadata": {},
      "outputs": [],
      "source": [
        "imp = model_imp(X_most_imp, y, model)\nprint(imp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b2885a-887b-4dd0-a9f9-518ef2a77d32",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h3>Question  #4:</h3>\n\n<b>Build a plot that shows the accuracy of a defined model as a function of the number of input features (using most important features) </b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "36342d25-61c8-4a30-8f23-ae51f648d1ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n\ncol = []\nac = []\nfor c in imp.index:\n    col.append(c)\n    ac.append(model_ac(X_most_imp[col], y_enc, model))\n    print('Input fields: ', len(col), 'Accuracy: %.2f' % (ac[-1]*100))\nac = pd.Series(ac)\nac.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cff1f69-d241-4e60-9ffe-45e01e33058a",
      "metadata": {},
      "outputs": [],
      "source": [
        "<details><summary>Click here for the solution</summary>\n\n```python\ncol = []\nac = []\nfor c in imp.index:\n    col.append(c)\n    ac.append(model_ac(X_most_imp[col], y_enc, model))\n    print('Input fields: ', len(col), 'Accuracy: %.2f' % (ac[-1]*100))\nac = pd.Series(ac)\nac.plot()\n```\n\n</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "668e5d00-d6cc-4173-a8fc-91d2ab6463c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Logistic regression <p id=\"logistic_regression\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "288a5828-6861-4b65-9b05-cc9a5519b163",
      "metadata": {},
      "outputs": [],
      "source": [
        "As you can see Accuracy of this model is very good.\n\nThere are many different techniques for scoring features and selecting features based on scores; how do you know which one to use?\n\nA robust approach is to evaluate models using different feature selection methods (and numbers of features) and select the method that results in a model with the best performance.\n\n**[Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** is a good model for testing feature selection methods as it can perform better if irrelevant features are removed from the model. We will use this model in absolutelly similar way like previous one."
      ]
    },
    {
      "cell_type": "code",
      "id": "1f0f4710-7452-47ae-a03f-7e8ef51b23f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(solver='lbfgs')\nmodel.fit(X_most_imp, y_enc)\nyhat = model.predict(X_most_imp)\naccuracy = accuracy_score(y_enc, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469b11a4-76c4-461b-8e55-87bac1a2c57d",
      "metadata": {},
      "outputs": [],
      "source": [
        "As we can see, the accuracy of the Logistic Regression model is lower (about 72%)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c31ccaf-c694-47a2-909f-051d3f550001",
      "metadata": {},
      "outputs": [],
      "source": [
        "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h3>Question  #5:</h3>\n    \n<b>Calculate the accuracy of the Logistic Regression model using all features</b>\n\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "id": "c6c6fdd8-bf2c-4da0-80b3-b4ffab9db238",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code below and press Shift+Enter to execute\n\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(x_enc, y)\nyhat = model.predict(x_enc)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9314a32-8994-4bb9-bec0-3927bcf50d39",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Visualization of decision tree <p id=\"visualization_of_decision_tree\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2e744b6-d57b-4260-a992-5b0620e57b9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's visualize the decision tree.\nThere are some ways to do it. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54019ce2-a0d2-4625-a07e-ab641aa13090",
      "metadata": {},
      "outputs": [],
      "source": [
        "Since when building the decision tree we will have many fields and because of this nothing will be visible, so we will set the limit max_depth = 3"
      ]
    },
    {
      "cell_type": "code",
      "id": "0148fcfa-b541-40d2-ada2-108a5753afa7",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dec = DecisionTreeClassifier(max_depth = 3)\nmodel_dec.fit(X_most_imp, y)\nyhat = model_dec.predict(X_most_imp)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac054845-82c5-47fd-aec5-7dc9c0c09acb",
      "metadata": {},
      "outputs": [],
      "source": [
        "Accuracy is quite high."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70c4a596-b683-4265-8b51-8aa1c301db00",
      "metadata": {},
      "outputs": [],
      "source": [
        "### _Text visualization_"
      ]
    },
    {
      "cell_type": "code",
      "id": "7c3831a0-fa2b-443f-911a-65c5e5c6ee85",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_representation = tree.export_text(model_dec, feature_names=list(X_most_imp.columns))\nprint(text_representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1f2ea2-665f-47bf-81c7-c921bb9397d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can save it into the file:"
      ]
    },
    {
      "cell_type": "code",
      "id": "5df6d67d-d2ea-4168-99d5-db9a5b28e504",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"decistion_tree.log\", \"w\") as fout:\n    fout.write(text_representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109fa546-1552-4d27-aeda-ca1cff115bd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "### _Plot tree_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8820d918-a579-40a4-9b0b-cfb60ed5cf21",
      "metadata": {},
      "outputs": [],
      "source": [
        "You can plot a tree using two different ways:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b7d067-9bbb-4a6d-8249-fb3b6762c2d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "**[plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)** (slow render - this can take some time): \n"
      ]
    },
    {
      "cell_type": "code",
      "id": "33e0b15d-86c3-4740-b98c-cff80fbef6a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model_dec,\n               feature_names = col,\n               filled = True)"
      ]
    },
    {
      "cell_type": "code",
      "id": "32ddba09-efb8-4e25-917e-c3e9b2cc99fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig.savefig('decision_tree.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c833a29f-b0b0-44d0-aa9c-0ce653231a5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "Or you can use **[python-graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html)** library. This is more fast function"
      ]
    },
    {
      "cell_type": "code",
      "id": "b40220c8-03bd-4833-b8c5-e0057401e79e",
      "metadata": {},
      "outputs": [],
      "source": [
        "dot_data = tree.export_graphviz(model_dec,\n               feature_names = col,\n               filled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc18eaf7-afd0-43c6-bd55-30fb97f8ccfa",
      "metadata": {},
      "outputs": [],
      "source": [
        "After creation, you can draw a graph"
      ]
    },
    {
      "cell_type": "code",
      "id": "6d832421-b233-4f4c-8802-350d3cba6c3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = graphviz.Source(dot_data, format=\"png\") \ngraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08094372-88ff-4207-844b-3e9051a4b310",
      "metadata": {},
      "outputs": [],
      "source": [
        "We can see that we have normalized data on the decision tree. An ordinary person will not be able to understand such data. So let's rebuild the decision tree and look at the real data."
      ]
    },
    {
      "cell_type": "code",
      "id": "7241dda9-f76f-4d6c-b945-0654e9629242",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_real = DecisionTreeClassifier(max_depth = 3)\nX_most_imp_real = df[X_most_imp.columns]\n\nmodel_real.fit(X_most_imp_real, y)\nyhat = model_real.predict(X_most_imp_real)\naccuracy = accuracy_score(y, yhat)\nprint('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e29ae6-de3c-4f30-8d29-2dfcf8d2e5a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "The accuracy is the same."
      ]
    },
    {
      "cell_type": "code",
      "id": "2068ed52-23c4-4236-bf8a-0a0ee155c2d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_representation = tree.export_text(model_real, feature_names=list(X_most_imp_real.columns))\nprint(text_representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6968bdae-5196-441b-96a0-309f1ade3e42",
      "metadata": {},
      "outputs": [],
      "source": [
        "Save to a file"
      ]
    },
    {
      "cell_type": "code",
      "id": "04e8b971-9307-4f42-b14b-cf02812a56ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"decistion_tree.log\", \"w\") as fout:\n    fout.write(text_representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9413a24-9164-4661-8578-99085a33f93e",
      "metadata": {},
      "outputs": [],
      "source": [
        "We build a decision tree using **[plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)**"
      ]
    },
    {
      "cell_type": "code",
      "id": "7d14e10c-6e5a-4320-90ba-fc7585c44a35",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model_real,\n               feature_names = list(X_most_imp_real.columns),\n               filled = True)"
      ]
    },
    {
      "cell_type": "code",
      "id": "ae7ce229-99ad-4bd0-b84e-f236a17f8e90",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig.savefig('decision_tree.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee3eaea-f24c-4877-ac8a-b262a6a92b07",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now build a decision tree using **[python-graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html)**"
      ]
    },
    {
      "cell_type": "code",
      "id": "10ed0079-c37d-4dec-9d26-735b5dfed544",
      "metadata": {},
      "outputs": [],
      "source": [
        "dot_data = tree.export_graphviz(model_real,\n               feature_names = list(X_most_imp_real.columns),\n               filled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e2f3e70-503a-4fa4-a04f-271507fae943",
      "metadata": {},
      "outputs": [],
      "source": [
        "Let's draw a graph"
      ]
    },
    {
      "cell_type": "code",
      "id": "83195158-8eab-4050-8308-1e441472e6e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = graphviz.Source(dot_data, format=\"png\") \ngraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2c46da-7e58-4fe8-b5ca-de9c8d608b74",
      "metadata": {},
      "outputs": [],
      "source": [
        "Now, let's test our decision tree.\n\nFor the test, we will take three patients with indices 1, 28, 40: \n\nPatient with index 1: Relapse Free Status-Recurred <= 0.50 - TRUE | Age at Diagnosis <= 64.01 - TRUE | Age at Diagnosis <= 53.76 - TRUE -> Result is **Living** in DataSet Patient's Vital Status value also **Living**.\n\nPatient with index 28: Relapse Free Status-Recurred <= 0.50 - FALSE | Overall Survival (Years) <= 10.43 - TRUE | Age at Diagnosis <= 59.26 - TRUE - > Result is **Died of Disease** in DataSet Patient's Vital Status value also **Died of Disease**.\n\nPatient with index 40: Relapse Free Status-Recurred <= 0.50 - FALSE | Overall Survival (Years) <= 10.43 - TRUE | Age at Diagnosis <= 59.26 - TRUE - > Result is **Died of Disease** in DataSet Patient's Vital Status value also **Died of Disease**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0b2696-a595-42b2-992e-1d4ef48203d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "And render it into the file:"
      ]
    },
    {
      "cell_type": "code",
      "id": "20fa992e-7a82-41ad-975a-6bab178f1bc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph.render(\"decision_tree_graphivz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25abd41-fd5e-4020-afc5-ce26e62085aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "Save the new csv:"
      ]
    },
    {
      "cell_type": "code",
      "id": "4e22e2a5-4c16-4046-8375-4a55c7b06412",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop([\"Patient ID\", \"ER Status\", \"Inferred Menopausal State-Pre\", \"Relapse Free Status-Recurred\", \"HER2 Status\"], axis=1)\ndf.to_csv('breast_cancer.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38025a2d-7738-4c8e-a498-1d3b65bf12c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7. Conclusions <p id=\"conclusions\"></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a07bf2-fa84-4b69-b51f-5b79f37b7b8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "In this lab, we learned to do preliminary data processing. In particular, change data types, and normalize and process categorical data. It was shown how to make feature selections by different methods. Learned how to build training and test DataSets. Shows how to work with different classifiers. It was also shown how to visualize a decision tree.\nAs a result of the lab, it was shown how based on a statistical database predict Patient Vital Status.\n\nThe Decision Tree and Extra Tree classifiers are highly accurate models for the given data. The accuracy of these models was found to be 100%, indicating that they are capable of accurately classifying the data. However, the accuracy of the Logistic Regression model was lower, at about 80%"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5c73a3-3a40-4392-85c7-277e1ca3996d",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Thank you for completing this lab!\n\n## Author\n\n<a href=\"https://author.skills.network/instructors/dmytro_shliakhovskyi\">Dmytro Shliakhovskyi</a>\n\n### Other Contributors\n\n<a href=\"https://author.skills.network/instructors/yaroslav_vyklyuk_2\">Prof. Yaroslav Vyklyuk, DrSc, PhD</a>\n\n<a href=\"https://author.skills.network/instructors/nataliya_boyko\">Ass. Prof. Nataliya Boyko, PhD</a>\n\n\n## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                         |\n| ----------------- | ------- | ---------- | ---------------------------------------------------------- |\n|    2023-03-18     | 01 | Dmytro Shliakhovkyi | Lab created |\n\n\n\n<hr>\n\n## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "",
      "display_name": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}